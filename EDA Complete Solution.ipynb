{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "640bd3f7",
   "metadata": {},
   "source": [
    "## Initial Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04ed2518",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "494d9d27",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"Basics\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a3d6550",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc5e0bec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this everytime you create a new spark instance. \n",
    "\n",
    "spark.sparkContext.install_pypi_package(\"plotly==5.5.0\")\n",
    "spark.sparkContext.install_pypi_package(\"pandas==0.25.1\")\n",
    "spark.sparkContext.install_pypi_package(\"numpy==1.14.5\")\n",
    "spark.sparkContext.install_pypi_package(\"matplotlib==3.1.1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d956d7bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.ml.feature import Bucketizer\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Import for typecasting columns\n",
    "from pyspark.sql.types import IntegerType,BooleanType,DateType,FloatType,StringType\n",
    "from pyspark.sql.types import ArrayType"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7c41efa",
   "metadata": {},
   "source": [
    "## Defining Custom Functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "922a9b82",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_quantiles(df, col_name, quantiles_list = [0.01, 0.25, 0.5, 0.75, 0.99]):\n",
    "    \"\"\"\n",
    "    Takes a numerical column and returns column values at requested quantiles\n",
    "\n",
    "    Inputs \n",
    "    Argument 1: Dataframe\n",
    "    Argument 2: Name of the column\n",
    "    Argument 3: A list of quantiles you want to find. Default value [0.01, 0.25, 0.5, 0.75, 0.99]\n",
    "\n",
    "    Output \n",
    "    Returns a dictionary with quantiles as keys and column quantile values as values \n",
    "    \"\"\"\n",
    "    # Get min, max and quantile values for given column\n",
    "    min_val = df.agg(F.min(col_name)).first()[0]\n",
    "    max_val = df.agg(F.max(col_name)).first()[0]\n",
    "    quantiles_vals = df.approxQuantile(col_name,\n",
    "                                       quantiles_list,\n",
    "                                       0)\n",
    "  \n",
    "    # Store min, quantiles and max in output dict, sequentially\n",
    "    quantiles_dict = {0.0:min_val}\n",
    "    quantiles_dict.update(dict(zip(quantiles_list, quantiles_vals)))\n",
    "    quantiles_dict.update({1.0:max_val})\n",
    "    return(quantiles_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da0aa680",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def plot_bucketwise_statistics (summary, bucketizer):\n",
    "    \"\"\"\n",
    "    Takes in a dataframe and a bucketizer object and plots the summary statistics for each bucket in the dataframe. \n",
    "  \n",
    "    Inputs\n",
    "    Argument 1: Pandas dataframe obtained from bucket_col_print_summary function \n",
    "    Argument 2: Bucketizer object obtained from bucket_col_print_summary function\n",
    "  \n",
    "    Output\n",
    "    Displays a plot of bucketwise average ratings nunber of ratings of a parameter.   \n",
    "    \"\"\"\n",
    "    # Creating bucket labels from splits\n",
    "    classlist = bucketizer.getSplits()\n",
    "    number_of_classes = len(classlist) - 1\n",
    "\n",
    "    class_labels = []\n",
    "    hover_labels = []\n",
    "    for i in range (number_of_classes):\n",
    "        hover_labels.append(str(classlist[i])+\"-\"+str(classlist[i+1]) +\" (Bucket name: \"+ str(int(i)) +\")\"  )\n",
    "        class_labels.append(str(classlist[i])+\"-\"+str(classlist[i+1]) )\n",
    "  \n",
    "    summary[\"Scaled_number\"] = (summary[\"n_ratings\"]-summary[\"n_ratings\"].min())/(summary[\"n_ratings\"].max()-summary[\"n_ratings\"].min()) + 1.5\n",
    "    summary['Bucket_Names'] = class_labels\n",
    "  \n",
    "    # making plot\n",
    "    x = summary[\"Bucket_Names\"]\n",
    "    y1 = summary[\"avg_rating\"]\n",
    "    y2 = summary[\"n_ratings\"]\n",
    "    err = summary[\"stddev_rating\"]  \n",
    "\n",
    "    # Plot scatter here\n",
    "    plt.rcParams[\"figure.figsize\"] = [summary.shape[0]+2, 6.0]\n",
    "    plt.rcParams[\"figure.autolayout\"] = True\n",
    "    fig, ax1 = plt.subplots()\n",
    "\n",
    "    bar = ax1.bar(x, y1, color = \"#262261\")\n",
    "    ax1.errorbar(x, y1, yerr=err, fmt=\"o\", color=\"#EE4036\")\n",
    "    ax1.set(ylim=(0, 7))\n",
    "  \n",
    "    #ax1.bar_label(bar , fmt='%.2f', label_type='edge')  \n",
    "    def barlabel(x_list,y_list):\n",
    "        for i in range(len(x_list)):\n",
    "            ax1.text(i,y_list[i] + 0.2,y_list[i], ha = 'center',\n",
    "  \t\t\t         fontdict=dict(size=10),\n",
    "  \t\t\t         bbox=dict(facecolor='#262261', alpha=0.2)         \n",
    "  \t\t\t        )\n",
    "    barlabel(summary[\"Bucket_Names\"].tolist() ,summary[\"avg_rating\"].round(2).tolist())\n",
    "  \n",
    "    ax2 = ax1.twinx()\n",
    "    ax2.scatter(x, y2, s=summary[\"Scaled_number\"]*500, c = '#FAAF40')  \n",
    "    ax2.set(ylim=(0, summary[\"n_ratings\"].max()*1.15))\n",
    "    def scatterlabel(x_list,y_list):\n",
    "  \t    for i in range(len(x_list)):\n",
    "  \t\t    ax2.text(i,y_list[i] + 15000,y_list[i], ha = 'center',\n",
    "  \t\t\t\t\t fontdict=dict(size=10),\n",
    "                     bbox=dict(facecolor='#FAAF40', alpha=0.5)\n",
    "  \t\t\t\t\t)\n",
    "    scatterlabel(summary[\"Bucket_Names\"].tolist() ,summary[\"n_ratings\"].tolist())\n",
    "  \n",
    "    # giving labels to the axises\n",
    "    ax1.set_xlabel(bucketizer.getOutputCol(), fontdict=dict(size=14)) \n",
    "    ax1.set_ylabel(\"Average Ratings\",fontdict=dict(size=14))\n",
    "  \n",
    "    # secondary y-axis label\n",
    "    ax2.set_ylabel('Number of Ratings',fontdict=dict(size=14))\n",
    "  \n",
    "    #plot Title\n",
    "    plt.title('Bucketwise average ratings and number of ratings for \\n'+bucketizer.getInputCol(), \n",
    "              fontdict=dict(size=14))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32067450",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bucket_col_print_summary(df, splits, inputCol, outputCol):\n",
    "    \"\"\"\n",
    "    Given a numerical column in a data frame, adds a bucketized version of the column to the data frame, according to splits provided.\n",
    "    Also prints a summary of ratings seen in each bucket made.\n",
    "\n",
    "    Inputs \n",
    "    Argument 1: Data Frame \n",
    "    Argument 2: Values at which the column will be split\n",
    "    Argument 3: Name of the input column (numerical column)\n",
    "    Argument 4: Name of the output column (bucketized numerical column)\n",
    "\n",
    "    Output: \n",
    "    1) New dataframe with the output column added\n",
    "    2) Bucketizer object trained from the input column \n",
    "    3) Pandas dataframe with summary statistics for ratings seen in buckets of the output column\n",
    "    Also plots summary statistics for ratings seen in buckets of the output column\n",
    "    \"\"\"\n",
    "\n",
    "    # Dropping bucket if it already exists\n",
    "    if outputCol in df.columns:\n",
    "        df = df.drop(outputCol)\n",
    "\n",
    "    # Training bucketizer\n",
    "    bucketizer = Bucketizer(splits = splits,\n",
    "                            inputCol  = inputCol,\n",
    "                            outputCol = outputCol)\n",
    "    \n",
    "    df = bucketizer.setHandleInvalid(\"keep\").transform(df)\n",
    "\n",
    "    # Printing meta information on buckets created\n",
    "    print(\"Added bucketized column {}\".format(outputCol))\n",
    "    print(\"\")\n",
    "    print(\"Bucketing done for split definition: {}\".format(splits))\n",
    "    print(\"\")  \n",
    "    print(\"Printing summary statistics for ratings in buckets below:\")\n",
    "\n",
    "    # Creating a summary statistics dataframe and passing it to the plotting function\n",
    "    summary =  (df\n",
    "                .groupBy(outputCol)\n",
    "                .agg(F.avg('rating').alias('avg_rating'),\n",
    "                     F.stddev('rating').alias('stddev_rating'),\n",
    "                     F.count('rating').alias('n_ratings'))\n",
    "                .sort(outputCol)\n",
    "                .toPandas())\n",
    "  \n",
    "    plot_bucketwise_statistics(summary,bucketizer)\n",
    "  \n",
    "    return df, bucketizer, summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2844741f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_column_distribution_summary(df, col_name):\n",
    "    \"\"\"\n",
    "    Takes a column in a data frame and prints the summary statistics (average, standard deviation, count and distinct count) for all unique values in that column.\n",
    "  \n",
    "    Inputs \n",
    "    Argument 1: Dataframe \n",
    "    Argument 2: Name of the column\n",
    "  \n",
    "    Output\n",
    "    Returns nothing \n",
    "    Prints a Dataframe with summary statistics\n",
    "    \"\"\"\n",
    "    print(df\n",
    "          .groupBy(col_name)\n",
    "          .agg(F.avg('rating').alias('avg_rating'),\n",
    "               F.stddev('rating').alias('stddev_rating'),\n",
    "               F.count('rating').alias('n_ratings'),\n",
    "               F.countDistinct('id').alias('n_recipes'))\n",
    "          .sort(F.col(col_name).asc())\n",
    "          .show(50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e4067ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_n_items_satisfying_condition (df, condition, aggregation_level = \"recipe\"):\n",
    "    \"\"\"\n",
    "    Given a condition, find the number of recipes / reviews that match the condition.\n",
    "    Also calculates the percentage of such recipes / reviews as a percentage of all recipes / reviews.\n",
    "  \n",
    "    Inputs \n",
    "    Argument 1: Dataframe \n",
    "    Argument 2: Logical expression describing a condition, string type. eg: \"minutes == 0\"\n",
    "    Argument 3: Aggregation level for determining \"items\", either  \"recipe\" or \"review\". Default value == \"recipe\"\n",
    "  \n",
    "    Output: Returns no object.\n",
    "    Prints the following:\n",
    "    1) Number of recipes / reviews that satisfy the condition\n",
    "    2) Total number of recipes / reviews in the dataframe\n",
    "    3) Percentage of recipes / reviews that satisfy the condition\n",
    "    \"\"\"\n",
    "    # Find out num rows satisfying the condition\n",
    "    if aggregation_level == \"recipe\": \n",
    "        number_of_rows_satisfying_condition = (df\n",
    "                                             .filter(condition)\n",
    "                                             .agg(F.countDistinct(\"id\"))).first()[0]\n",
    "      \n",
    "        n_rows_total = (df.agg(F.countDistinct(\"id\"))).first()[0]\n",
    "    if aggregation_level == \"review\":\n",
    "        number_of_rows_satisfying_condition = (df\n",
    "                                             .filter(condition)\n",
    "                                             .agg(F.countDistinct(\"id\",\"user_id\"))).first()[0]\n",
    "        n_rows_total = (df.agg(F.countDistinct(\"id\",\"user_id\"))).first()[0]\n",
    "  \n",
    "    # Find out % rows satisfying the conditon and print a properly formatted output\n",
    "    perc_rows = round(number_of_rows_satisfying_condition * 100/ n_rows_total, 2)\n",
    "    print('Condition String                   : \"{}\"'.format(condition))\n",
    "    print(\"Num {}s Satisfying Condition   : {} [{}%]\".format(aggregation_level.title(), number_of_rows_satisfying_condition, perc_rows))\n",
    "    print(\"Total Num {}s                  : {}\".format(aggregation_level.title(), n_rows_total))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eefe5257",
   "metadata": {},
   "source": [
    "## Read the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ca2574e",
   "metadata": {},
   "source": [
    "- Read ```interaction_level_df_processed```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "701e66a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "interaction_level_df = spark.read.parquet(\"s3://biplabstudio/Recipe_Recommender/Parquet file/\", header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7acf36fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code check cell\n",
    "# Do not edit cells with assert commands\n",
    "# If an error is shown after running this cell, please recheck your code.  \n",
    "\n",
    "assert interaction_level_df.count() == 1132367, \"There is a mistake in reading the data.\"\n",
    "assert len(interaction_level_df.columns) == 33, \"There is a mistake in reading the data.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ff3ae16",
   "metadata": {},
   "outputs": [],
   "source": [
    "interaction_level_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb8336fb",
   "metadata": {},
   "source": [
    "## Bucketing and Cleaning Numerical Features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a259fc10",
   "metadata": {},
   "source": [
    "#### **1. `years_since_submission_on_review_date`** \n",
    "[Review Time Since Submission]\n",
    "- Recipes more than 6 years old are rated low"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9739318a",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_quantiles(df = interaction_level_df,\n",
    "              col_name = \"years_since_submission_on_review_date\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c5d0a1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_n_items_satisfying_condition(df = interaction_level_df,\n",
    "                                 condition= 'years_since_submission_on_review_date < 0',\n",
    "                                 aggregation_level= \"review\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8015a401",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only keep interactions with review dates >= recipe submission date\n",
    "\n",
    "interaction_level_df = (interaction_level_df\n",
    "                        .filter('years_since_submission_on_review_date >= 0'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cd92e5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "splits = [ 0, 1, 3, 6, float('Inf')]\n",
    "inputCol  = \"years_since_submission_on_review_date\"\n",
    "outputCol = \"years_since_submission_on_review_date_bucket\"\n",
    "\n",
    "(interaction_level_df, submission_time_bucketizer, submission_time_pandas_df) = bucket_col_print_summary(df = interaction_level_df,\n",
    "                                                                              splits = splits,\n",
    "                                                                              inputCol  = inputCol,\n",
    "                                                                              outputCol = outputCol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2395619f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplot plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ee68eb7",
   "metadata": {},
   "source": [
    "#### **2. `minutes`** \n",
    "\n",
    "[prep time]\n",
    "- Somewhat relevant\n",
    "- Low prep time is preferred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2038a3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_quantiles(df = interaction_level_df,\n",
    "              col_name = \"minutes\",\n",
    "              quantiles_list=[0.01, 0.05, 0.25, 0.5, 0.75, 0.95, 0.99])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49e1b9d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Capping prep time at 930 minutes\n",
    "\n",
    "interaction_level_df = (interaction_level_df\n",
    "                        .withColumn(\"minutes\",\n",
    "                                    F.when(interaction_level_df[\"minutes\"] > 930, 930)\n",
    "                                     .otherwise(interaction_level_df[\"minutes\"])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9efcda9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# investigating recipes with minutes = 0 -> Look at n_steps for such recipes.\n",
    "\n",
    "get_column_distribution_summary(df = (interaction_level_df\n",
    "                                      .filter('minutes == 0')\n",
    "                                      .withColumn('n_steps_modified', (F.when(interaction_level_df['n_steps'] >= 10, \">= 10\")\n",
    "                                                                        .otherwise(F.lpad(interaction_level_df['n_steps'],2,\"0\"))))),\n",
    "                                col_name = 'n_steps_modified')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "509cf653",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's look at some examples with 1 step only to see if this makes sense\n",
    "\n",
    "interaction_level_df.filter('minutes == 0 and n_steps == 1').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd0656d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_n_items_satisfying_condition(df = interaction_level_df,\n",
    "                                 condition = 'minutes == 0',\n",
    "                                 aggregation_level = \"recipe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03e3fd59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove recipes with cook time zero\n",
    "\n",
    "interaction_level_df = interaction_level_df.filter(\"minutes > 0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "603439c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "splits = [0, 5, 15, 30, 60, 300, 900, float('Inf')]\n",
    "inputCol  = \"minutes\"\n",
    "outputCol = \"prep_time_bucket\"\n",
    "\n",
    "(interaction_level_df, prep_time_bucketizer, prep_time_summary_pandas_df) = bucket_col_print_summary(df = interaction_level_df,\n",
    "                                                                              splits = splits,\n",
    "                                                                              inputCol  = inputCol,\n",
    "                                                                              outputCol = outputCol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d040184",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplot plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3cb6116",
   "metadata": {},
   "source": [
    "**3. `n_steps`**\n",
    "\n",
    "- Clearly relevant\n",
    "- Recipes with less than 2 steps are rated high\n",
    "- Recipes with more than 29 steps are rated very low"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "653f7324",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_quantiles(df = interaction_level_df,\n",
    "              col_name = \"n_steps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f4acf93",
   "metadata": {},
   "outputs": [],
   "source": [
    "interaction_level_df.filter('n_steps == 0').show(5, truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22af25b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_n_items_satisfying_condition(df = interaction_level_df,\n",
    "                                 condition = 'n_steps == 0',\n",
    "                                 aggregation_level = \"recipe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de44d004",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove recipes with n_steps zero\n",
    "\n",
    "interaction_level_df = interaction_level_df.filter(\"n_steps > 0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df340d09",
   "metadata": {},
   "outputs": [],
   "source": [
    "splits = [0, 2, 6, 8, 12, 29, float('Inf')]\n",
    "inputCol  = \"n_steps\"\n",
    "outputCol = \"n_steps_bucket\"\n",
    "\n",
    "(interaction_level_df, n_steps_bucketizer, n_steps_pandas_df) = bucket_col_print_summary(df = interaction_level_df,\n",
    "                                                                              splits = splits,\n",
    "                                                                              inputCol  = inputCol,\n",
    "                                                                              outputCol = outputCol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14ded29c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplot plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6332e3c3",
   "metadata": {},
   "source": [
    "**4. `n_ingredients`**\n",
    "- Not relevant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce8e1030",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_quantiles(df = interaction_level_df,\n",
    "              col_name = \"n_ingredients\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7ed08af",
   "metadata": {},
   "outputs": [],
   "source": [
    "splits = [0, 6, 9, 11, float('Inf')]\n",
    "inputCol  = \"n_ingredients\"\n",
    "outputCol = \"n_ingredients_bucket\"\n",
    "\n",
    "(interaction_level_df, n_ingredients_bucketizer, n_ingredients_pandas_df) = bucket_col_print_summary(df = interaction_level_df,\n",
    "                                                                              splits = splits,\n",
    "                                                                              inputCol  = inputCol,\n",
    "                                                                              outputCol = outputCol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5dc708c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplot plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4dadbbf",
   "metadata": {},
   "source": [
    "**5. `nutrition` columns**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46ec0b11",
   "metadata": {},
   "source": [
    "- `calories` - Calories per serving seems irrelevant\n",
    "- `fat (per 100 cal)` - Calories per serving seems irrelevant\n",
    "- `sugar (per 100 cal)` - Calories per serving seems irrelevant\n",
    "- `sodium (per 100 cal)` - Calories per serving seems irrelevant\n",
    "- `protein (per 100 cal)` - Calories per serving seems irrelevant\n",
    "- `sat. fat (per 100 cal)` - Calories per serving seems irrelevant\n",
    "- `carbs (per 100 cal)` - Calories per serving seems irrelevant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "177938af",
   "metadata": {},
   "outputs": [],
   "source": [
    "interaction_level_df.columns \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5403a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "nutrition_cols = ['calories', \n",
    "                  'total_fat_PDV', \n",
    "                  'sugar_PDV', \n",
    "                  'sodium_PDV', \n",
    "                  'protein_PDV', \n",
    "                  'saturated_fat_PDV', \n",
    "                  'carbohydrates_PDV', \n",
    "                  'total_fat_per_100_cal', \n",
    "                  'sugar_per_100_cal', \n",
    "                  'sodium_per_100_cal', \n",
    "                  'protein_per_100_cal', \n",
    "                  'saturated_fat_per_100_cal', \n",
    "                  'carbohydrates_per_100_cal']\n",
    "\n",
    "quantiles_list = [0.00, 0.05, 0.25, 0.5, 0.75, 0.95, 1.00]\n",
    "nutrition_col_quantiles = pd.DataFrame(index = quantiles_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea51b34e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in nutrition_cols:\n",
    "    nutrition_col_quantiles[col] = (get_quantiles(df = interaction_level_df,\n",
    "                                                col_name = col,\n",
    "                                                quantiles_list=quantiles_list)\n",
    "                                  .values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3ee770e",
   "metadata": {},
   "outputs": [],
   "source": [
    "nutrition_col_quantile_summary = pd.DataFrame(index = [\"0.00-0.25\", \"0.25-0.50\", \"0.50-0.75\", \"0.75-0.95\", \"0.95 - 1.00\"])\n",
    "\n",
    "for col in nutrition_cols:\n",
    "    splits = ([0]\n",
    "            + list(nutrition_col_quantiles.loc[[0.25, 0.5, 0.75, 0.95], col].round())\n",
    "            + [float('Inf')])\n",
    "    inputCol  = col\n",
    "    outputCol = col+\"_bucket\"\n",
    "\n",
    "    if outputCol in interaction_level_df.columns:\n",
    "        interaction_level_df = interaction_level_df.drop(outputCol)\n",
    "\n",
    "  # Training bucketizer\n",
    "    bucketizer = Bucketizer(splits = splits,\n",
    "                          inputCol  = inputCol,\n",
    "                          outputCol = outputCol)\n",
    "  \n",
    "    interaction_level_df = bucketizer.setHandleInvalid(\"keep\").transform(interaction_level_df)\n",
    "  \n",
    "    nutrition_col_quantile_summary.loc[:, col] = (interaction_level_df\n",
    "                                                .groupBy(outputCol)\n",
    "                                                .agg(F.avg('rating').alias('avg_rating'))\n",
    "                                                .sort(outputCol)\n",
    "                                                .select('avg_rating').toPandas().values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10ea621d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the max columns to none\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19d2bd55",
   "metadata": {},
   "outputs": [],
   "source": [
    "nutrition_col_quantile_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81fb2d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(interaction_level_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eab7800",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Writing the modified data to S3 \n",
    "\n",
    "interaction_level_df.write.mode('overwrite').parquet('s3://biplabstudio/Recipe_Recommender/Parquet file/')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
